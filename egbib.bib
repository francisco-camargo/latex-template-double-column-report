@article{BossardFood101,
	address = {Long Beach, CA, USA},
	title = {Food-101 - {Mining Discriminative Components with Random Forests}},
	isbn = {9781728132938},
	shorttitle = {Food-101},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-10599-4_29},
	doi = {10.1007/978-3-319-10599-4_29},
	urldate = {2021-07-24},
	publisher = {Springer, Cham},
    booktitle = {European Conference on Computer Vision},
	author = {Bossard, Luke and Guillaumin, Matthieu and Gool, Luc Van},
	year = {2014},
	pages = {446--461},
}

@article{salvador2017learning,
  title={Learning Cross-modal Embeddings for Cooking Recipes and Food Images},
  author={Salvador, Amaia and Hynes, Nicholas and Aytar, Yusuf and Marin, Javier and 
          Ofli, Ferda and Weber, Ingmar and Torralba, Antonio},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2017}
}

@article{marin2019learning,
  title = {Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images},
  author = {Marin, Javier and Biswas, Aritro and Ofli, Ferda and Hynes, Nicholas and 
  Salvador, Amaia and Aytar, Yusuf and Weber, Ingmar and Torralba, Antonio},
  journal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
  year = {2019}
}

@article{2019fb,
	address = {Long Beach, CA, USA},
	title = {Inverse {Cooking}: {Recipe} {Generation} {From} {Food} {Images}},
	isbn = {9781728132938},
	shorttitle = {Inverse {Cooking}},
	url = {https://ieeexplore.ieee.org/document/8953192/},
	doi = {10.1109/CVPR.2019.01070},
	urldate = {2021-07-24},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Salvador, Amaia and Drozdzal, Michal and Giro-i-Nieto, Xavier and Romero, Adriana},
	month = jun,
	year = {2019},
	pages = {10445--10454},
}

@article{2021MIT,
	title = {{Recipe1M}+: {A} {Dataset} for {Learning} {Cross}-{Modal} {Embeddings} for {Cooking} {Recipes} and {Food} {Images}},
	volume = {43},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {{Recipe1M}+},
	url = {https://ieeexplore.ieee.org/document/8758197/},
	doi = {10.1109/TPAMI.2019.2927476},
	number = {1},
	urldate = {2021-08-01},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Marin, Javier and Biswas, Aritro and Ofli, Ferda and Hynes, Nicholas and Salvador, Amaia and Aytar, Yusuf and Weber, Ingmar and Torralba, Antonio},
	month = jan,
	year = {2021},
	pages = {187--203},
}

@misc{recipe1m_data,
	title = {{Recipe1M}+: {A} {Dataset} for {Learning} {Cross}-{Modal} {Embeddings} for {Cooking} {Recipes} and {Food} {Images} - {MIT}},
	url = {http://pic2recipe.csail.mit.edu/},
	urldate = {2021-08-01},
}

@article{lin_focal_2018,
	title = {Focal {Loss} for {Dense} {Object} {Detection}},
	url = {http://arxiv.org/abs/1708.02002},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	urldate = {2021-07-30},
	journal = {arXiv:1708.02002 [cs]},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll√°r, Piotr},
	month = feb,
	year = {2018},
	note = {arXiv: 1708.02002},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{lowe_distinctive_2004,
	title = {Distinctive {Image} {Features} from {Scale}-{Invariant} {Keypoints}},
	volume = {60},
	issn = {1573-1405},
	url = {https://doi.org/10.1023/B:VISI.0000029664.99615.94},
	doi = {10.1023/B:VISI.0000029664.99615.94},
	abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
	language = {en},
	number = {2},
	urldate = {2021-07-31},
	journal = {International Journal of Computer Vision},
	author = {Lowe, David G.},
	month = nov,
	year = {2004},
	pages = {91--110},
}

@inproceedings{arandjelovic_three_2012,
	address = {USA},
	series = {{CVPR} '12},
	title = {Three things everyone should know to improve object retrieval},
	isbn = {9781467312264},
	abstract = {The objective of this work is object retrieval in large scale image datasets, where the object is specified by an image query and retrieval should be immediate at run time in the manner of Video Google [28]. We make the following three contributions: (i) a new method to compare SIFT descriptors (RootSIFT) which yields superior performance without increasing processing or storage requirements; (ii) a novel method for query expansion where a richer model for the query is learnt discriminatively in a form suited to immediate retrieval through efficient use of the inverted index; (iii) an improvement of the image augmentation method proposed by Turcot and Lowe [29], where only the augmenting features which are spatially consistent with the augmented image are kept. We evaluate these three methods over a number of standard benchmark datasets (Oxford Buildings 5k and 105k, and Paris 6k) and demonstrate substantial improvements in retrieval performance whilst maintaining immediate retrieval speeds. Combining these complementary methods achieves a new state-of-the-art performance on these datasets.},
	urldate = {2021-07-31},
	booktitle = {Proceedings of the 2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE Computer Society},
	author = {Arandjelovic, Relja},
	month = jun,
	year = {2012},
	keywords = {Vectors, Euclidean distance, Kernel, Support vector machines, Indexes, Visualization, Standards},
	pages = {2911--2918},
}

@misc{opencv_flann,
	title = {{OpenCV}: {Feature} {Matching} with {FLANN}},
	url = {https://docs.opencv.org/3.4/d5/d6f/tutorial_feature_flann_matcher.html},
	urldate = {2021-07-31},
}

@article{sift_lowe,
    title = {Distinctive Image Features from Scale-Invariant Keypoints},
    url = {},
    urldate = {2021-08-01},
    journal = {International Journal of Computer Vision},
    year = {2004},
    pages = {91-110},
    author = {Lowe, David},
    doi = {10.1023/B:VISI.0000029664.99615.94}
    
}

@article{lindeberg_scale_2012,
	title = {Scale invariant feature transform},
	volume = {7},
	doi = {10.4249/scholarpedia.10491},
	number = {5},
	journal = {Scholarpedia},
	author = {Lindeberg, Tony},
	year = {2012},
	pages = {10491},
}

@article{chen_chinesefoodnet_2017,
	title = {{ChineseFoodNet}: {A} large-scale {Image} {Dataset} for {Chinese} {Food} {Recognition}},
	shorttitle = {{ChineseFoodNet}},
	url = {http://arxiv.org/abs/1705.02743},
	abstract = {In this paper, we introduce a new and challenging large-scale food image dataset called "ChineseFoodNet", which aims to automatically recognizing pictured Chinese dishes. Most of the existing food image datasets collected food images either from recipe pictures or selfie. In our dataset, images of each food category of our dataset consists of not only web recipe and menu pictures but photos taken from real dishes, recipe and menu as well. ChineseFoodNet contains over 180,000 food photos of 208 categories, with each category covering a large variations in presentations of same Chinese food. We present our efforts to build this large-scale image dataset, including food category selection, data collection, and data clean and label, in particular how to use machine learning methods to reduce manual labeling work that is an expensive process. We share a detailed benchmark of several state-of-the-art deep convolutional neural networks (CNNs) on ChineseFoodNet. We further propose a novel two-step data fusion approach referred as "TastyNet", which combines prediction results from different CNNs with voting method. Our proposed approach achieves top-1 accuracies of 81.43\% on the validation set and 81.55\% on the test set, respectively. The latest dataset is public available for research and can be achieved at https://sites.google.com/view/chinesefoodnet.},
	urldate = {2021-07-31},
	journal = {arXiv:1705.02743 [cs]},
	author = {Chen, Xin and Zhu, Yu and Zhou, Hua and Diao, Liang and Wang, Dongyan},
	month = oct,
	year = {2017},
	note = {arXiv: 1705.02743
version: 3},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{rezatofighi_generalized_2019,
	title = {Generalized {Intersection} over {Union}: {A} {Metric} and {A} {Loss} for {Bounding} {Box} {Regression}},
	shorttitle = {Generalized {Intersection} over {Union}},
	url = {http://arxiv.org/abs/1902.09630},
	abstract = {Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks. However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value. The optimal objective for a metric is the metric itself. In the case of axis-aligned 2D bounding boxes, it can be shown that \$IoU\$ can be directly used as a regression loss. However, \$IoU\$ has a plateau making it infeasible to optimize in the case of non-overlapping bounding boxes. In this paper, we address the weaknesses of \$IoU\$ by introducing a generalized version as both a new loss and a new metric. By incorporating this generalized \$IoU\$ (\$GIoU\$) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, \$IoU\$ based, and new, \$GIoU\$ based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO.},
	urldate = {2021-07-31},
	journal = {arXiv:1902.09630 [cs]},
	author = {Rezatofighi, Hamid and Tsoi, Nathan and Gwak, JunYoung and Sadeghian, Amir and Reid, Ian and Savarese, Silvio},
	month = apr,
	year = {2019},
	note = {arXiv: 1902.09630},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{OpenCV_blur,
	title = {Gaussian {Blur} - {OpenCV}},
	url = {https://docs.opencv.org/4.5.2/d4/d13/tutorial_py_filtering.html},
	urldate = {2021-07-31},
}

@article{su_one_2019,
	title = {One pixel attack for fooling deep neural networks},
	volume = {23},
	issn = {1089-778X, 1089-778X, 1941-0026},
	url = {http://arxiv.org/abs/1710.08864},
	doi = {10.1109/TEVC.2019.2890858},
	abstract = {Recent research has revealed that the output of Deep Neural Networks (DNN) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution (DE). It requires less adversarial information (a black-box attack) and can fool more types of networks due to the inherent features of DE. The results show that 67.97\% of the natural images in Kaggle CIFAR-10 test dataset and 16.04\% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least one target class by modifying just one pixel with 74.03\% and 22.91\% confidence on average. We also show the same vulnerability on the original CIFAR-10 dataset. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks. Besides, we also illustrate an important application of DE (or broadly speaking, evolutionary computation) in the domain of adversarial machine learning: creating tools that can effectively generate low-cost adversarial attacks against neural networks for evaluating robustness.},
	number = {5},
	urldate = {2021-08-01},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {Su, Jiawei and Vargas, Danilo Vasconcellos and Kouichi, Sakurai},
	month = oct,
	year = {2019},
	note = {arXiv: 1710.08864},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	pages = {828--841},
}

@book{gonzalez_digital_2008,
	address = {Upper Saddle River, N.J},
	edition = {3rd ed},
	title = {Digital image processing},
	isbn = {9780131687288},
	publisher = {Prentice Hall},
	author = {Gonzalez, Rafael C. and Woods, Richard E.},
	year = {2008},
	keywords = {Image processing, Digital techniques},
}

@inproceedings{he_deep_2016,
	address = {Las Vegas, NV, USA},
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	isbn = {9781467388511},
	url = {http://ieeexplore.ieee.org/document/7780459/},
	doi = {10.1109/CVPR.2016.90},
	urldate = {2021-08-01},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	pages = {770--778},
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2021-08-01},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}



@misc{mit_github,
	title = {{GitHub} - torralba-lab/im2recipe-{Pytorch}: im2recipe {Pytorch} implementation},
	shorttitle = {{GitHub} - torralba-lab/im2recipe-{Pytorch}},
	url = {https://github.com/torralba-lab/im2recipe-Pytorch},
	abstract = {im2recipe Pytorch implementation. Contribute to torralba-lab/im2recipe-Pytorch development by creating an account on GitHub.},
	language = {en},
	urldate = {2021-08-01},
	journal = {GitHub},
}

@misc{fb_github,
	title = {{GitHub} - facebookresearch/inversecooking: {Recipe} {Generation} from {Food} {Images}},
	shorttitle = {{GitHub} - facebookresearch/inversecooking},
	url = {https://github.com/facebookresearch/inversecooking},
	abstract = {Recipe Generation from Food Images. Contribute to facebookresearch/inversecooking development by creating an account on GitHub.},
	language = {en},
	urldate = {2021-08-02},
	journal = {GitHub},
}

@article{gatys_neural_2015,
	title = {A {Neural} {Algorithm} of {Artistic} {Style}},
	url = {http://arxiv.org/abs/1508.06576},
	abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
	urldate = {2021-08-02},
	journal = {arXiv:1508.06576 [cs, q-bio]},
	author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
	month = sep,
	year = {2015},
	note = {arXiv: 1508.06576},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
}